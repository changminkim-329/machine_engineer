{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc54072f",
   "metadata": {},
   "source": [
    "# 딥러닝 모델 서빙과 병렬처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3320c18",
   "metadata": {},
   "source": [
    "## 병렬처리\n",
    "- [참조](/notebooks/머신러닝엔지니어/병렬처리/병렬처리.ipynb)\n",
    "- 프로세스는 컴퓨터에서 연속적으로 실행되고 있는 컴퓨터 프로그램\n",
    "- 쓰레드는 프로세스 내에서 실행되는 흐름의 단위\n",
    "- 병렬처리는 여러 쓰레드를 활용하는 방법이 있고, 여러 개의 프로세스를 활용하는 방법이 있으며 각각 **멀티 쓰레드**, **멀티 프로세싱**이라고 부름.\n",
    "- 멀티 프로세싱은 여러 개의 프로세스가 별도로 실행되고 각 프로세스가 별개의 메모리를 차지하고 있으며, 멀티 쓰레드는 하나의 프로세스 내에서 메모리를 공유해서 사용.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f437a12",
   "metadata": {},
   "source": [
    "### python의 GIL\n",
    "- [참조](https://changminkimserver29672.iptime.org:62911/notebooks/머신러닝엔지니어/병렬처리/파이썬_GIL.ipynb#Race-condition-문제)\n",
    "- Global Interpreter Lock\n",
    "- 파이썬 인터프리터가 한 스레드만 하나의 바이트코드를 실행 시킬 수 있도록 해주는 **Mutex**\n",
    "- 하나의 스레드에 모든 자원을 허락하고 그 후에는 Lock을 걸어 다른 스레드는 실행할 수 없게 막아버리는 것\n",
    "- 즉 파이썬 쓰레드는 한번에 하나 밖에 동작을 못 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb10550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e43310b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def working():\n",
    "    max([random.random() for i in range(20000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd4ad51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.20173\n"
     ]
    }
   ],
   "source": [
    "# 1 Thread\n",
    "s_time = time.time()\n",
    "working()\n",
    "working()\n",
    "e_time = time.time()\n",
    "print(f'{e_time - s_time:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69dafc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.29190\n"
     ]
    }
   ],
   "source": [
    "# 2 Threads\n",
    "s_time = time.time()\n",
    "threads = []\n",
    "for i in range(2):\n",
    "    threads.append(threading.Thread(target=working))\n",
    "    threads[-1].start()\n",
    "\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "e_time = time.time()\n",
    "print(f'{e_time - s_time:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b46cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3085dd97",
   "metadata": {},
   "source": [
    "## Tensorflow 와 Pytorch의 병렬처리\n",
    "Tensorflow 나 Pytorch 같은 경우 코어 내부에 Python이 아닌 다른 언어가 동작하고 있어서 GIL의 영향을 받지 않고, inference를 수행하는 동안 **멀티 쓰레드**가 동작할 수 있다.\n",
    "\n",
    "그래서 Flask를 활용하여 서빙할 때, 실행 시에 threaded=True를 사용하면, 요청이 동시에 들어와도 수행이 가능.\n",
    "\n",
    "하지만 pytorch의 경우에는 현재 멀티 쓰레드를 사용하면 내부적으로 변수가 꼬이는 현상이 발생하므로 사용하시면 안된다.\n",
    "\n",
    "Pytorch의 경우 **멀티 프로세싱**을 활용하면 되는데, 멀티 프로세싱은 여러 개의 프로세스를 실행시키는 방식으로, 각 프로세스가 별도로 동작하므로 pytorch에서 아무 문제가 없이 사용가능.\n",
    "\n",
    "사실, tensorflow의 경우에도 멀티 쓰레드로 처리하는 것 보다, 멀티 프로세싱으로 처리하는 것이 더 빠르다.\n",
    "\n",
    "멀티 프로세스를 사용한다고 하면 인퍼런스를 하는 서버를 여러 개 띄워두고, 요청이 들어올 때는 잘 중재하여 각 인퍼런스 서버에 나눠서 일을 시키도록 하는 모습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3660fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efd612f8",
   "metadata": {},
   "source": [
    "### Flask를 이용한 멀티 프로세스로 딥러닝 모델 서빙.\n",
    "- Gunicorn같은 미들웨어를 활용해서 멀티 프로세스 구현.\n",
    "- pip install gunicorn\n",
    "- gunicorn flask서버파일:app --bind=0.0.0.0:포트 -w 워커개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6690d3",
   "metadata": {},
   "source": [
    "### Server 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa72dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from flask import Flask, request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e13f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load = tf.saved_model.load('../model/mnist/1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d65c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_inference = load.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af2f1cda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:41253\n",
      " * Running on http://172.17.0.4:41253\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "192.168.0.1 - - [23/Oct/2022 13:50:23] \"POST /inference HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dense': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[3.5300819e-04, 6.5266178e-03, 7.9976125e-03, 2.4240771e-03,\n",
      "        4.7246707e-03, 3.5549065e-03, 1.8990660e-04, 9.6926939e-01,\n",
      "        2.2097756e-03, 2.7500829e-03]], dtype=float32)>}\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.0.1 - - [23/Oct/2022 13:50:46] \"POST /inference HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dense': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[3.5300819e-04, 6.5266178e-03, 7.9976125e-03, 2.4240771e-03,\n",
      "        4.7246707e-03, 3.5549065e-03, 1.8990660e-04, 9.6926939e-01,\n",
      "        2.2097756e-03, 2.7500829e-03]], dtype=float32)>}\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.0.1 - - [23/Oct/2022 13:50:57] \"POST /inference HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dense': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[3.5300819e-04, 6.5266178e-03, 7.9976125e-03, 2.4240771e-03,\n",
      "        4.7246707e-03, 3.5549065e-03, 1.8990660e-04, 9.6926939e-01,\n",
      "        2.2097756e-03, 2.7500829e-03]], dtype=float32)>}\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.0.1 - - [23/Oct/2022 13:51:07] \"POST /inference HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dense': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[8.4653095e-04, 2.8771220e-03, 1.7690003e-02, 4.4492311e-03,\n",
      "        4.6465406e-03, 5.0320975e-03, 9.1941254e-03, 6.5094652e-04,\n",
      "        9.5343238e-01, 1.1809780e-03]], dtype=float32)>}\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/inference',methods=['POST'])\n",
    "def inference():\n",
    "    global data\n",
    "    data = request.json\n",
    "    image = data['images']\n",
    "    pre_image = tf.reshape(tf.constant(image,dtype=tf.float32)/255.0,(1,28,28,1))\n",
    "    result = load_inference(pre_image)\n",
    "    print(result)\n",
    "    print(str(np.argmax(result['dense'].numpy())))\n",
    "    return str(np.argmax(result['dense'].numpy()))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0',port='41253')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54d7a37",
   "metadata": {},
   "source": [
    "### gunciron 실행\n",
    "- worker는 4개로 지정.(4개의 프로세스에서 나눠서 병렬로 처리)\n",
    "- --daemon 옵션을 추가하면 백그라운드에서 실행가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gunicorn flask_server:app --bind=0.0.0.0:41253 -w 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95f23f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gunicorn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgunicorn\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gunicorn' is not defined"
     ]
    }
   ],
   "source": [
    "gunicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6318a60c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
